---
layout: post
title:  How we moved from a reactive to proactive support model
tags: [leadership, bugs]
---
  
# And then there were none


## Team Lead
I have been working on the "support" team at Mavenlink since March of this year and and was promoted to team lead in September. During my time on the team I have been pushing to get our bug count from a rolling ~100 bugs on our monolith down to 0. Since I started on the team until now the team's bug count went from a high of 112 to now a rolling count of ~10 and a total team count of about 40 right now. Becoming team lead has given me the ability to really focus on how I thought we should crush the bugs. 

### Phase 0 - Setup the process
During my time on the team before I became team lead I worked closely with the current team lead. She started a process that had only been talked about before. The idea that we could start redistributing bugs to the teams that actually created them. It seems so simple now but the idea of a bug team can be too tempting for many organizations. This is not a novel idea but it can be difficult to do once a team takes the "support team" approach. 

Tim Otter has a great article that I found after this whole process was complete (http://agileotter.blogspot.com/2014/01/bug-teams-well-meaning-foolishness.html) that talks through why support teams are a bad idea. The idea that most companies have when this starts is that if we push all of the bugs to a team that just handles bugs we will have more time to build new features! `Everyone loves features so this is really going to speed things up for us.` - People with a bad idea. In the end this causes a lot more problems than it solves. So the first thing we did was setup what teams own what area of the code. This helped us create buy in from product teams and setup ownership from the start. This was one of the key parts to this whole thing. Getting buy in from all of the teams. We made a lot of tough choices when picking who owned what and the support team ended up owning some of the most difficult parts of the system, but hey better just some of the difficult parts than the whole application. In addition to setting this team ownership map up we also started to pick off some of the easiest bugs in the backlog. We spent a lot of the week working on difficult bugs that took a long time and were never able to get some of the easier bugs finished. So on Fridays we spent the day getting as many of the small bugs as we could done. This changed helped the current team feel like we were really making a dent in the backlog.  

### Phase 1 - Get the bug count to 0 
Getting the bug count to 0 is nearly impossible task if you have a large application with lots of users. We were getting lots of new bugs reported every week and were not really able to keep up with the demand so we had to do something new. This included the Friday bug bashes for our team but also was a lot of time grooming the backlog for bugs that were so old the feature did not exist of it was fixed by something else. We even just had bugs that we never intended to fix or they were intended behavior of the application. So we had a lot of great weeks early on just removing non-issues. This is really important to get the team to buy in and excited about the count down to 0. Now the whole goal is not really just a 0 bug count but we will talk about that later. 

### Phase 2 - Setup proactive monitoring
I was lucky enough to join a team that had new relic setup in the application. This enabled us to see what 500's were being thrown and any other issues that might be unexpected such as slow page loads. New relic was vital to the next step of our process because it enabled us to see a lot of the bugs that were reported before the end user even reported them. An important part of getting useful information from New relic is to reduce noise in the logs. This meant spending a lot of time understanding what errors you could "ignore" and what were real problems. Understanding the errors that were just noise meant we could move those errors to metrics in New relic which is just a different way to reocrd information. We did this to errors that came up a lot but were not new namely deadlocks and duplicate index errors due to race conditions. This greatly reduced the amount of errors we were getting in but we still recorded the incident with metrics. This is a vital part of understanding your application without losing useful information. Errors are unexpected behavior in an application and should be noted and fixed, metrics on the other hand are used to understand your application better. In our case we want to understand patterns around when and were race conditions for unique index constraints are occuring but we do not need to be alrted that it happened. Our database handled the bad data properly but our application was not and let the error bubble all the way up.

### Phase 3 - Proactive support
This phase of the process is still on going but it has involved setting up better dashboards and alerts in New relic for our application. We are now sending errors to our main developer slack channel to alert the whole team that an error occurred. The support team is still doing a lot of the triage for these errors but we have buy in from the other teams and they will fix the issues when they pop up. We are getting about 5-10 errors per day now and are quickly closing in on having 0 errors per day. 

### Key points
 - Clear out the backlog as much as you can. Remove bugs that are not actionable and fix the easy ones. 
 - Get your error logs to be as useful as possible by reducing noise. 
 - Send errors to a slack channel that is used the whole team so it is painful. We wanted it to be annoying so it would push people to fix problems. Make sure you reduce noise BEFORE you turn something like this on so people pay attention to these errors!
   
